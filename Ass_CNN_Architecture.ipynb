{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647dfdf3-1ec0-4b16-8edf-729cf3f314bd",
   "metadata": {},
   "source": [
    "# TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc2bd1-fcf0-47aa-a08e-75157abe6389",
   "metadata": {},
   "source": [
    "## 1. Describe the purpose and benefits of pooling in CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb3b64-51d0-4381-9d4a-679f51d375e5",
   "metadata": {},
   "source": [
    "Pooling is a fundamental operation in Convolutional Neural Networks (CNNs) that plays a crucial role in reducing the spatial dimensions of the input volume. The primary purpose of pooling is to progressively reduce the spatial size of the representation to decrease the amount of computation in the network and to control overfitting.\n",
    "\n",
    "Here are the main purposes and benefits of pooling in CNNs:\n",
    "\n",
    "- Spatial Hierarchical Representation:\n",
    "Pooling helps in creating a spatial hierarchy in the network by progressively reducing the size of the feature maps. This allows the network to focus on capturing the most essential features in higher-level layers while discarding less relevant spatial information.\n",
    "- Parameter Reduction and Computational Efficiency:\n",
    "Pooling reduces the number of parameters and computations in the network. By downsampling the spatial dimensions, the subsequent layers have fewer parameters to learn, making the network more computationally efficient and reducing the risk of overfitting, especially in cases where the training data is limited.\n",
    "- Translation Invariance:\n",
    "Pooling enhances the network's ability to achieve a degree of translation invariance. By summarizing local information through pooling, the network becomes less sensitive to small variations in the input, making the learned features more robust to different translations of the same pattern.\n",
    "- Memory Efficiency:\n",
    "Pooling reduces the memory requirements during training and inference. Smaller feature maps after pooling operations require less memory, which is particularly important for resource-constrained environments, such as mobile devices or embedded systems.\n",
    "- Increased Receptive Field:\n",
    "As pooling reduces the spatial dimensions, each unit in the pooled feature map covers a larger receptive field in the original input. This means that each unit in the higher layers is influenced by a larger portion of the input, allowing the network to capture more global patterns.\n",
    "\n",
    "Two common types of pooling used in CNNs are Max Pooling and Average Pooling. Max Pooling takes the maximum value from a group of values, while Average Pooling computes the average. Both types serve similar purposes but can have slightly different effects on the learned representations. Pooling is typically applied after convolutional layers in CNN architectures, and its parameters (such as pool size and stride) are hyperparameters that can be tuned based on the specific requirements of the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8db967-c758-4edb-96de-a849bf9e2515",
   "metadata": {},
   "source": [
    "## 2. Explain the difference between min pooling and max pooling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d58bff-d462-479e-9307-8c7b9bfc1883",
   "metadata": {},
   "source": [
    "Min pooling and max pooling are both types of pooling operations used in Convolutional Neural Networks (CNNs) to downsample the spatial dimensions of the input feature maps. The key difference lies in how they aggregate information from the local regions:\n",
    "\n",
    "1. Max Pooling:\n",
    "- Operation: In max pooling, for each local region (pooling window), the maximum value is retained and used to represent that region in the downsampled feature map.\n",
    "- Effect: Max pooling focuses on capturing the most prominent feature within a local region. It helps the network retain the most significant information while discarding less important details.\n",
    "- Advantages:\n",
    "Effective for capturing the most distinctive features.\n",
    "Provides a degree of translation invariance.\n",
    "2. Min Pooling:\n",
    "- Operation: In min pooling, the minimum value within each local region is retained and used as the representative value for that region in the downsampled feature map.\n",
    "- Effect: Min pooling tends to emphasize the least intense features within a local region. It may be less commonly used than max pooling, as it can be more sensitive to noise and less effective in capturing the most discriminative features.\n",
    "- Advantages:\n",
    "Can be useful in specific cases where the minimum values carry relevant information.\n",
    "\n",
    "In summary, the main distinction is in the aggregation function applied to the local regions. Max pooling focuses on the maximum value, which is often useful for capturing strong, distinctive features. On the other hand, min pooling focuses on the minimum value, which may be applied in scenarios where the least intense features are considered important. Max pooling is more widely used in practice, but the choice between max pooling and min pooling depends on the characteristics of the data and the specific requirements of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0acfb8e-51cf-4255-9487-47ddb6e5239b",
   "metadata": {},
   "source": [
    "## 3. Discuss the concept of padding in CNN and its significance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab185d-60d5-4ce9-b7d6-2d6027e46b74",
   "metadata": {},
   "source": [
    "Padding is a technique used in Convolutional Neural Networks (CNNs) to add extra pixels around the input data before applying convolution operations. Padding involves adding zeros or other constant values to the input matrix, effectively increasing its size. The primary purpose of padding is to control the spatial dimensions of the output feature maps and mitigate issues that arise at the edges of the input data during convolution operations.\n",
    "\n",
    "Here are the key concepts and significance of padding in CNNs:\n",
    "- Preventing Dimension Reduction:\n",
    "During convolution operations, the spatial dimensions of the feature maps tend to decrease. Without padding, as the convolutional layers progress through the network, the spatial dimensions can shrink rapidly, leading to a loss of information at the edges. Padding helps maintain the spatial size of the feature maps, preventing excessive reduction.\n",
    "- Preserving Spatial Information:\n",
    "Padding ensures that the convolutional filters can process the pixels at the borders of the input data, preserving spatial information. This is crucial for maintaining the integrity of the features at the edges of objects in the image, which might be otherwise ignored without padding.\n",
    "- Handling Border Effects:\n",
    "When convolving a filter with the input data, the filter is usually centered on a pixel. At the edges, this means that only part of the filter overlaps with the input, which can lead to border effects. Padding mitigates this issue by providing a buffer around the input, allowing the filter to fully cover all regions of the input.\n",
    "- Ensuring Consistent Output Size:\n",
    "Padding is often used to ensure that the output feature maps have the same spatial dimensions as the input, especially when strides greater than 1 are used. This consistency in size simplifies the design of neural network architectures and makes it easier to stack multiple layers.\n",
    "- Facilitating Information Flow:\n",
    "Padding helps maintain a more uniform flow of information across layers. This is particularly important in deep networks where the spatial dimensions can decrease rapidly. Consistent padding allows the network to capture both local and global information effectively.\n",
    "\n",
    "Two common types of padding are zero-padding and valid (or no) padding. Zero-padding involves adding zeros around the input matrix, while valid padding means no padding is added. The amount of padding is a hyperparameter that can be adjusted based on the specific requirements of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92ca6b-d81c-447e-9b32-c472a6884f2b",
   "metadata": {},
   "source": [
    "## 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7809b9-40d7-4887-b86a-914be08e4103",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding are two common approaches to handle the spatial dimensions of the output feature maps in Convolutional Neural Networks (CNNs). Let's compare and contrast these two types of padding in terms of their effects on the output feature map size:\n",
    "\n",
    "1. Zero-padding:\n",
    "- Operation: Zero-padding involves adding zeros around the input matrix before applying convolution operations.\n",
    "- Effect on Output Size:\n",
    "  - Increased Output Size: Zero-padding increases the spatial dimensions of the input, effectively expanding its size. The additional pixels at the borders allow convolutional filters to process information at the edges of the input.\n",
    "  - Consistent Output Size: By adding zeros, zero-padding helps in maintaining a consistent output size, especially when using strides greater than 1. This consistency simplifies the design of neural network architectures.\n",
    "2. Valid-padding:\n",
    "- Operation: Valid-padding, also known as no padding, means no extra pixels are added around the input before convolution.\n",
    "- Effect on Output Size:\n",
    "  - Reduced Output Size: Without padding, the convolutional filters only process the pixels that entirely fit within the input, leading to a reduction in spatial dimensions. The output feature map size is smaller compared to the input size.\n",
    "  - Border Effects: Valid-padding can result in border effects, where the convolutional filters do not fully cover the pixels at the edges of the input, potentially leading to loss of information.\n",
    "\n",
    "Comparison:\n",
    "1. Spatial Dimensions:\n",
    "- Zero-padding increases the spatial dimensions of the input, ensuring that the convolutional filters cover all regions of the input.\n",
    "- Valid-padding reduces the spatial dimensions of the input, potentially leading to information loss at the edges.\n",
    "2. Consistency:\n",
    "- Zero-padding provides a consistent output size, which can simplify the design of neural network architectures.\n",
    "- Valid-padding may result in varying output sizes, depending on the size of the input and the convolutional filter dimensions.\n",
    "3. Border Effects:\n",
    "- Zero-padding mitigates border effects by allowing convolutional filters to fully cover the input, especially at the edges.\n",
    "- Valid-padding may lead to border effects as the filters may not fully cover the pixels at the input borders.\n",
    "\n",
    "Use Cases:\n",
    "- Zero-padding: Often used when maintaining spatial information at the edges is crucial, or when a consistent output size is desired.\n",
    "- Valid-padding: May be used when dimensionality reduction is acceptable, and the network is designed to handle border effects effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7299bb5-1d83-4651-91d6-d4298d5014b1",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e37aa4-8ea8-4e23-9313-c23bb817315b",
   "metadata": {},
   "source": [
    "## 1. Provide a brief overview of Lenet-5 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5cec4-18d3-4404-8e43-efc805de6496",
   "metadata": {},
   "source": [
    "It was introduced in 1998 and played a significant role in popularizing the use of CNNs for handwritten digit recognition tasks, particularly in the context of the MNIST dataset.\n",
    "\n",
    "Here is a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "1. Input Layer:\n",
    "- LeNet-5 takes as input grayscale images of size 32x32 pixels. The original design was intended for handwritten digit recognition.\n",
    "2. First Convolutional Layer (C1):\n",
    "  - The first convolutional layer consists of six feature maps (also called channels or kernels).\n",
    "  - Convolution is applied with a 5x5 kernel with a stride of 1.\n",
    "  - A sigmoid activation function is applied to the output of each convolutional operation.\n",
    "  - Subsampling (average pooling) is performed with a 2x2 window and a stride of 2, reducing the spatial dimensions.\n",
    "3. Second Convolutional Layer (C3):\n",
    "  - C3 is another convolutional layer with 16 feature maps.\n",
    "  - 5x5 convolutional kernels are applied with a stride of 1.\n",
    "  - Sigmoid activation function is applied.\n",
    "  - Subsampling (average pooling) is performed with a 2x2 window and a stride of 2.\n",
    "4. Third Fully Connected Layer (F4):\n",
    "  - F4 is a fully connected layer with 120 neurons.\n",
    "  - Sigmoid activation is applied.\n",
    "5. Fourth Fully Connected Layer (F5):\n",
    "  - F5 is another fully connected layer with 84 neurons.\n",
    "  - Sigmoid activation is applied.\n",
    "6. Output Layer:\n",
    "- The output layer consists of 10 neurons, representing the digits 0 through 9.\n",
    "- A softmax activation function is applied to produce probability scores for each class.\n",
    "7. Activation Function:\n",
    "Sigmoid activation functions were commonly used throughout the network in the original LeNet-5 architecture.\n",
    "8. Training:\n",
    "- The network was trained using the backpropagation algorithm with stochastic gradient descent (SGD).\n",
    "9. Loss Function:\n",
    "Cross-entropy loss was typically used for training LeNet-5 on classification tasks.\n",
    "\n",
    "LeNet-5 was groundbreaking in its time and demonstrated the effectiveness of deep learning for image recognition tasks. While its architecture is relatively simple compared to modern CNNs, it laid the foundation for subsequent advancements in deep learning and convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e958bb-039d-4737-981d-9b26005666ca",
   "metadata": {},
   "source": [
    "## 2. Describe the key components of Lenet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b061b8b8-0c76-47f4-9556-61e9f3cc0217",
   "metadata": {},
   "source": [
    "LeNet-5 consists of several key components, each serving a specific purpose in the architecture. Here are the main components of LeNet-5 and their respective purposes:\n",
    "\n",
    "1. Input Layer:\n",
    "- Purpose: The input layer takes grayscale images of size 32x32 pixels. It serves as the initial stage for feeding input data into the network.\n",
    "2. First Convolutional Layer (C1):\n",
    "- Purpose:\n",
    "  - Extracts low-level features through convolutional operations using a 5x5 kernel.\n",
    "  - Introduces non-linearity through the application of a sigmoid activation function.\n",
    "  - Subsamples the feature maps using average pooling with a 2x2 window and a stride of 2, reducing spatial dimensions.\n",
    "- Outcome: Six feature maps capturing basic patterns.\n",
    "2. Second Convolutional Layer (C3):\n",
    "- Purpose:\n",
    "  - Further extracts higher-level features through additional convolutional operations.\n",
    "  - Applies a sigmoid activation function for non-linearity.\n",
    "  - Subsamples feature maps using average pooling with a 2x2 window and a stride of 2.\n",
    "- Outcome: Sixteen feature maps capturing more complex patterns.\n",
    "3. Third Fully Connected Layer (F4):\n",
    "- Purpose:\n",
    "  - Transforms the spatially organized features from the convolutional layers into a flat vector.\n",
    "  - Connects each unit to every unit in the previous layer.\n",
    "  - Applies a sigmoid activation function for non-linearity.\n",
    "- Outcome: 120 neurons capturing abstract features.\n",
    "4. Fourth Fully Connected Layer (F5):\n",
    "- Purpose:\n",
    "  - Further processes the abstract features obtained from the previous layer.\n",
    "  - Connects each unit to every unit in the previous layer.\n",
    "  - Applies a sigmoid activation function for non-linearity.\n",
    "- Outcome: 84 neurons capturing more abstract features.\n",
    "5. Output Layer:\n",
    "- Purpose:\n",
    "  - Represents the output layer responsible for classification.\n",
    "  - Consists of 10 neurons corresponding to the digits 0 through 9.\n",
    "  - Applies a softmax activation function to produce probability scores for each class.\n",
    "- Outcome: Probability distribution over the 10 digit classes.\n",
    "6. Activation Function (Sigmoid):\n",
    "- Purpose: The sigmoid activation function introduces non-linearity to the network, allowing it to learn complex relationships and representations in the data.\n",
    "7. Training with Backpropagation:\n",
    "- Purpose: The network is trained using the backpropagation algorithm with stochastic gradient descent (SGD). This involves adjusting the weights and biases of the network to minimize a specified loss function.\n",
    "8. Loss Function (Cross-Entropy):\n",
    "- Purpose: Cross-entropy loss is typically used for training LeNet-5 on classification tasks. It measures the difference between the predicted probabilities and the true labels, guiding the network to improve its predictions during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f25e79-c89b-4cd2-b60b-43f4041c2919",
   "metadata": {},
   "source": [
    "## 3. Discuss the advantages and limitations of Lenet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4175e73-ceae-4eef-b231-be846743c386",
   "metadata": {},
   "source": [
    "Advantages of LeNet-5:\n",
    "1. Pioneering Architecture:\n",
    "LeNet-5 was one of the first successful implementations of a convolutional neural network for image recognition tasks, specifically handwritten digit recognition. It demonstrated the potential of deep learning in computer vision.\n",
    "2. Effective Feature Extraction:\n",
    "The convolutional layers in LeNet-5 perform hierarchical feature extraction, capturing low-level and high-level features progressively. This enables the network to learn meaningful representations from the input data.\n",
    "3. Spatial Hierarchical Representation:\n",
    "The architecture's use of convolutional and subsampling layers creates a spatial hierarchy in the network, allowing it to capture both local and global features in an image.\n",
    "4. Parameter Sharing:\n",
    "The convolutional layers in LeNet-5 use parameter sharing, meaning that the same weights are applied to different regions of the input. This reduces the number of parameters in the network, making it computationally efficient and reducing the risk of overfitting.\n",
    "5. Demonstrated Success on MNIST:\n",
    "LeNet-5 achieved state-of-the-art performance on the MNIST dataset, showcasing its effectiveness in handwritten digit recognition.\n",
    "\n",
    "Limitations of LeNet-5:\n",
    "1. Limited Model Complexity:\n",
    "Compared to modern deep learning architectures, LeNet-5 has a relatively simple structure. For more complex image classification tasks with intricate patterns, deeper and more sophisticated networks may be required.\n",
    "2. Sigmoid Activation Function:\n",
    "LeNet-5 uses the sigmoid activation function, which has limitations such as the vanishing gradient problem. Modern architectures often use rectified linear units (ReLUs) or other activation functions that mitigate these issues.\n",
    "3. Small Input Size:\n",
    "The 32x32 input size of LeNet-5 may be considered small for certain high-resolution image classification tasks. Modern architectures often handle larger input sizes, allowing them to capture more detailed information.\n",
    "4. Limited Flexibility:\n",
    "LeNet-5 was specifically designed for handwritten digit recognition, and its architecture may not be as versatile for other types of image classification tasks without modifications.\n",
    "5. Pooling Overlapping Information:\n",
    "The pooling layers in LeNet-5 perform subsampling with a fixed window size and stride, potentially causing the network to overlook certain spatial patterns due to overlapping pooling windows.\n",
    "6. Less Robust to Variability:\n",
    "LeNet-5 may be less robust to variations in object appearance, orientation, and scale compared to more advanced architectures designed to handle a broader range of image classification challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b0f3c-eedf-4c52-8e63-b9f668344242",
   "metadata": {},
   "source": [
    "## 4. Implement LeNet-5 using a deep learning framework oj your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95648056-61bb-4ebf-a298-f235e1e09c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.4 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.2 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba5ce0d-ef30-4ff1-8e03-0b436aa2f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 13:33:39.628819: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 13:33:39.696591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 13:33:39.696665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 13:33:39.698536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 13:33:39.708500: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 13:33:39.709226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 13:33:40.902706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D,MaxPooling2D,AveragePooling2D,Dense,Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82878b0a-8cad-4396-98fd-c40450316965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 18s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62006 (242.21 KB)\n",
      "Trainable params: 62006 (242.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - 7s 15ms/step - loss: 1.8429 - accuracy: 0.3434 - val_loss: 1.7180 - val_accuracy: 0.3941\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 6s 14ms/step - loss: 1.6697 - accuracy: 0.4120 - val_loss: 1.6081 - val_accuracy: 0.4260\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train),(X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train,10)\n",
    "y_test = keras.utils.to_categorical(y_test,10)\n",
    "\n",
    "# Building the Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6,kernel_size=(5,5),padding='valid',activation='tanh',input_shape=(32,32,3)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(16,kernel_size=(5,5),padding='valid',activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120,activation='tanh'))\n",
    "model.add(Dense(84,activation='tanh'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128,epochs=2,verbose=1,validation_data=(X_test,y_test))\n",
    "score = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test Loss:',score[0])\n",
    "print('Test accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb87fe1-681f-46a7-87b0-b386ee082792",
   "metadata": {},
   "source": [
    "# Topic: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd577840-01de-478e-b6fa-416603a5323c",
   "metadata": {},
   "source": [
    "## 1. Present an overview of the Alexnet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6ddaf-49fb-43d4-80e8-7acb7ded0741",
   "metadata": {},
   "source": [
    "1. Architecture:\n",
    "- AlexNet consists of five convolutional layers followed by three fully connected layers.\n",
    "- It uses the Rectified Linear Unit (ReLU) activation function throughout the network, except in the output layer where softmax is employed for classification.\n",
    "2. Input:\n",
    "- The network takes an RGB image as input, with a fixed size of 227x227 pixels.\n",
    "3. Convolutional Layers:\n",
    "- The first convolutional layer has 96 kernels of size 11x11 with a stride of 4 pixels.\n",
    "- The second convolutional layer has 256 kernels of size 5x5, and it is followed by a max-pooling layer with a size of 3x3 and a stride of 2 pixels.\n",
    "- The third, fourth, and fifth convolutional layers have 384, 384, and 256 kernels of size 3x3, respectively. The last two layers are also followed by max-pooling layers.\n",
    "4. Fully Connected Layers:\n",
    "- The three fully connected layers have 4096 neurons each.\n",
    "- The first two fully connected layers are followed by dropout layers with a dropout probability of 0.5, which helps prevent overfitting.\n",
    "- The output layer has 1000 neurons corresponding to the 1000 ImageNet classes.\n",
    "5. Normalization and Local Response Normalization (LRN):\n",
    "- LRN is applied after the first and second convolutional layers. It normalizes the responses across neighboring channels to enhance the model's generalization.\n",
    "6. Activation Function:\n",
    "- The Rectified Linear Unit (ReLU) activation function is used in all layers, except the output layer.\n",
    "7. Training:\n",
    "- AlexNet was trained using the stochastic gradient descent (SGD) optimization algorithm.\n",
    "- Data augmentation techniques, such as random cropping and horizontal flipping, were employed to artificially increase the size of the training dataset and improve generalization.\n",
    "8. Achievements:\n",
    "- AlexNet significantly outperformed previous methods in the ILSVRC 2012, reducing the top-5 error rate by a considerable margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e04be-45df-4f30-897a-5b29340e5da3",
   "metadata": {},
   "source": [
    "## 2. Explain the architectural innovations introduced in Alexnet that contributed to its breakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf673b1-f2a6-46d4-9ff2-7ca45355061f",
   "metadata": {},
   "source": [
    "1. Deep Architecture:\n",
    "- AlexNet was one of the first deep convolutional neural networks (CNNs) to have a significant depth, with a total of eight layers (five convolutional and three fully connected layers). The depth of the network allowed it to learn complex hierarchical features from raw pixel values.\n",
    "2. Large Convolutional Kernels:\n",
    "- The first convolutional layer in AlexNet used large 11x11 filters with a stride of 4 pixels. This large filter size helped capture coarse features in the input images, allowing the network to learn low-level representations effectively.\n",
    "3. Local Response Normalization (LRN):\n",
    "- LRN was applied after the first and second convolutional layers. LRN normalizes the responses across neighboring channels, promoting competition between different feature maps. This helps enhance the contrast between activated features and improves the model's ability to generalize.\n",
    "4. ReLU Activation Function:\n",
    "- AlexNet used the Rectified Linear Unit (ReLU) activation function throughout the network, except in the output layer. ReLU helps address the vanishing gradient problem, allowing for faster convergence during training compared to traditional activation functions like sigmoid or tanh.\n",
    "5. Overlapping Max Pooling:\n",
    "- AlexNet employed overlapping max pooling, which means that the pooling regions overlapped, reducing the loss of spatial resolution. This allowed the network to retain more spatial information, making it more robust to object translations in the input images.\n",
    "6. Data Augmentation:\n",
    "- The training of AlexNet involved data augmentation techniques, such as random cropping and horizontal flipping. Data augmentation helped artificially increase the size of the training dataset, reducing overfitting and improving the model's ability to generalize to new, unseen data.\n",
    "7. Dropout:\n",
    "- AlexNet used dropout in the fully connected layers. Dropout is a regularization technique that randomly drops out (sets to zero) a fraction of neurons during training. This helps prevent overfitting and encourages the network to learn more robust and diverse features.\n",
    "8. GPU Acceleration:\n",
    "- Training deep neural networks, especially with a large number of parameters, can be computationally intensive. AlexNet was one of the early models to leverage GPU acceleration for training, which significantly reduced training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2ecda-1dd2-41e2-a31a-abe8cd2e1337",
   "metadata": {},
   "source": [
    "## 3. Discuss the role of convolutional layers,pooling layers, and fully connected layers in Alexnet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9147e-ad47-4719-9b24-990303c117be",
   "metadata": {},
   "source": [
    "In AlexNet, the architecture is composed of convolutional layers, pooling layers, and fully connected layers. Each of these layer types plays a specific role in the overall functioning of the network, contributing to the model's ability to learn hierarchical features from input images.\n",
    "\n",
    "1. Convolutional Layers:\n",
    "- Role: Convolutional layers are responsible for learning and extracting features from the input images. They use convolutional operations to apply filters (kernels) to small portions of the input image, capturing local patterns and structures.\n",
    "- Innovation in AlexNet: AlexNet introduced large convolutional kernels, such as 11x11 in the first layer, to capture more global features and spatial hierarchies. This was a departure from previous architectures with smaller kernels.\n",
    "2. Pooling Layers:\n",
    "- Role: Pooling layers, specifically max pooling in AlexNet, downsample the spatial dimensions of the feature maps generated by the convolutional layers. This reduces the computational complexity of the network, makes the learned features more translation-invariant, and helps in creating a form of spatial hierarchy.\n",
    "- Innovation in AlexNet: AlexNet used overlapping max pooling, which means that the pooling regions overlapped, allowing for better preservation of spatial information compared to traditional non-overlapping pooling.\n",
    "3. Fully Connected Layers:\n",
    "- Role: Fully connected layers take the high-level features learned by the convolutional and pooling layers and combine them to make predictions. In the case of image classification, these layers are responsible for producing the final output scores for different classes.\n",
    "- Innovation in AlexNet: AlexNet had three fully connected layers with a large number of neurons (4096 each). The use of fully connected layers with a large number of parameters allows the model to learn complex relationships and representations, contributing to its capacity to understand and discriminate between different object classes.\n",
    "\n",
    "In summary, the convolutional layers serve as feature extractors, capturing patterns and structures in the input images. Pooling layers reduce spatial dimensions, making the learned features more robust and computationally efficient. Fully connected layers combine these features to make final predictions. The innovative aspects of AlexNet, such as the use of large convolutional kernels, overlapping pooling, and a deep architecture with a large number of parameters, contributed to its breakthrough performance in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51c3a5-fab3-467f-818b-d3850f265e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
